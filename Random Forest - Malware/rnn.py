'''
import os
import sys
import shutil
import errno
import time
import random
import fnmatch
import numpy as np
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import csv
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import model_selection
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import confusion_matrix
#from xgboost import XGBClassifier

# combined is 100 lass a and b together
# testadv is not github class a 
# adversarial is bad adversarial that doesn't work
# adv-branch is adversarial ttack with no cache attck trainedo n 9,10 / 17 ,19
#classbsamples class b files for recall

dataset = pd.read_csv('train.csv') # train chstone/mibench/spec with 100 x86 malware

x = dataset.iloc[:, [0,1,2,3]].values
y = dataset.iloc[:, 20].values

print(x)
print(y)


X_train,X_test, Y_train, Y_test = train_test_split(x,y, test_size= 0.20)

scaler = StandardScaler()  
scaler.fit(X_train)

X_train = scaler.transform(X_train)  
X_test = scaler.transform(X_test)  


print("----first---------------------------------")
#clf = tree.DecisionTreeClassifier()
#clf = RandomForestClassifier()
#clf = XGBClassifier()
#clf = RandomForestClassifier(max_features=5)
clf = RandomForestClassifier(max_depth=100,n_estimators=100)
#clf = MLPClassifier()
#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50,50,50),max_iter=300, random_state=1)
clf.fit(X_train,Y_train)
y_pred = clf.predict(X_test)
print("-----------------second----------------------")
print("Accuracy: ", np.mean(y_pred==Y_test))


	


#print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))
#print(precision_recall_fscore_support(Y_test, y_pred))

# accuracy: (tp + tn) / (p + n)
print("\n")
print("real values \n")


accuracy1 = accuracy_score(y_pred, Y_test)
print('Accuracy: %f' % accuracy1)
# precision tp / (tp + fp)
precision1 = precision_score(y_pred, Y_test)
print('Precision: %f' % precision1)
# recall: tp / (tp + fn)
recall1 = recall_score(y_pred, Y_test)
print('Recall: %f' % recall1)
# f1: 2 tp / (2 tp + fp + fn)
#f1 = f1_score(y_pred, Y_test)
#print('F1 score: %f' % f1)



print('-------------cm------------------')
cm = confusion_matrix(Y_test,y_pred)
#cm
print(cm)

print('------------endcm-------------------')


'''



#RNN--------------------------------------------------------------------------------------------------------------------





'''



import os
import sys
import shutil
import errno
import time
import random
import fnmatch
import numpy as np
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import csv
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import model_selection
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import confusion_matrix


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM#, CuDNNLSTM













dataset = pd.read_csv('train.csv') # train chstone/mibench/spec with 100 x86 malware

x = dataset.iloc[:, [0,1,2,3]].values
y = dataset.iloc[:, 20].values

print(x)
print(y)


x_train,x_test, y_train, y_test = train_test_split(x,y, test_size= 0.20)

#scaler = StandardScaler()  
#scaler.fit(x_train)

#x_train = scaler.transform(x_train)  
#x_test = scaler.transform(x_test)  


#x_train = x_train/255.0
#x_test = x_test/255.0

print(x_train.shape)
print(x_train[0].shape)

model = Sequential()

# IF you are running with a GPU, try out the CuDNNLSTM layer type instead (don't pass an activation, tanh is required)
model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(128, activation='relu'))
model.add(Dropout(0.1))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))

opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy'],
)

model.fit(x_train,
          y_train,
          epochs=3,
          validation_data=(x_test, y_test))


'''


import numpy
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
import pandas as pd
# fix random seed for reproducibility
#numpy.random.seed(7)
# load the dataset but only keep the top n words, zero the rest








dataset = pd.read_csv('train.csv') # train chstone/mibench/spec with 100 x86 malware

x = dataset.iloc[:, [0,1,2,3]].values
y = dataset.iloc[:, 20].values

print(x)
print(y)
train = []
trainans = []

counter = 1
temp1 = []
temp2 = []
temp3 = []
temp4 = []
for i in range(0,200):
	temp1.append(x[counter - 1][0])
	temp2.append(x[counter - 1][1])
	temp3.append(x[counter - 1][2])
	temp4.append(x[counter - 1][3])
	if counter % 100 == 0:
		#print(temp1)
		#print("entered ", counter)
		train.append(temp1)
		#print(train)
		train.append(temp2)
		train.append(temp3)
		train.append(temp4)
		trainans.append(y[counter - 1])
		trainans.append(y[counter - 1])
		trainans.append(y[counter - 1])
		trainans.append(y[counter - 1])
		temp1 = []
		temp2 = []
		temp3 = []
		temp4 = []
	counter += 1

print(train)
print(trainans)
	
train = numpy.reshape(train,(-1,100))
print(train)
trainans = numpy.reshape(trainans,(-1,1))
print(trainans)

traintest = train[1]
traintestans = trainans[0]


#x_train,x_test, y_train, y_test = train_test_split(x,y, test_size= 0.20)

#scaler = StandardScaler()  
#scaler.fit(x_train)

#x_train = scaler.transform(x_train)  
#x_test = scaler.transform(x_test)  

#top_words = 5000
top_words = 100

#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

print("---------------------------------------------------------")
#print(X_train)
# truncate and pad input sequences

#max_review_length = 500
max_review_length = 100
#X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
#X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)
# create the model
embedding_vecor_length = 100
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(LSTM(100))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(train, trainans, epochs=1, batch_size=64)
# Final evaluation of the model
scores = model.evaluate(traintest, traintestans, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))





