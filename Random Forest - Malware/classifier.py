
import os
import sys
import shutil
import errno
import time
import random
import fnmatch
import numpy as np
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import csv
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import precision_recall_fscore_support
from sklearn import model_selection
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import confusion_matrix
#from xgboost import XGBClassifier

# combined is 100 lass a and b together
# testadv is not github class a 
# adversarial is bad adversarial that doesn't work
# adv-branch is adversarial ttack with no cache attck trainedo n 9,10 / 17 ,19
#classbsamples class b files for recall

dataset = pd.read_csv('train.csv') # train chstone/mibench/spec with 100 x86 malware

x = dataset.iloc[:, [0,1,2,3]].values
y = dataset.iloc[:, 20].values

print(x)
print(y)


X_train,X_test, Y_train, Y_test = train_test_split(x,y, test_size= 0.20)

scaler = StandardScaler()  
scaler.fit(X_train)

X_train = scaler.transform(X_train)  
X_test = scaler.transform(X_test)  


print("----first---------------------------------")
clf = tree.DecisionTreeClassifier()
#clf = RandomForestClassifier()
#clf = XGBClassifier()
#clf = RandomForestClassifier(max_features=5)
#clf = RandomForestClassifier(max_depth=100,n_estimators=100)
#clf = MLPClassifier()
#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50,50,50),max_iter=300, random_state=1)
clf.fit(X_train,Y_train)
y_pred = clf.predict(X_test)
print("-----------------second----------------------")
print("Accuracy: ", np.mean(y_pred==Y_test))


	


#print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))
#print(precision_recall_fscore_support(Y_test, y_pred))

# accuracy: (tp + tn) / (p + n)
print("\n")
print("real values \n")


accuracy1 = accuracy_score(y_pred, Y_test)
print('Accuracy: %f' % accuracy1)
# precision tp / (tp + fp)
precision1 = precision_score(y_pred, Y_test)
print('Precision: %f' % precision1)
# recall: tp / (tp + fn)
recall1 = recall_score(y_pred, Y_test)
print('Recall: %f' % recall1)
# f1: 2 tp / (2 tp + fp + fn)
#f1 = f1_score(y_pred, Y_test)
#print('F1 score: %f' % f1)



print('-------------cm------------------')
cm = confusion_matrix(Y_test,y_pred)
#cm
print(cm)

print('------------endcm-------------------')



